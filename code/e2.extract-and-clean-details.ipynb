{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b15142",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import csv\n",
    "import json\n",
    "import gzip \n",
    "import spacy\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "\n",
    "from datetime import datetime\n",
    "from typing import Dict, List\n",
    "from dateutil.parser import parse\n",
    "from unidecode import unidecode\n",
    "from collections import Counter\n",
    "from tqdm.notebook import tqdm\n",
    "from rapidfuzz.distance import Levenshtein\n",
    "from rapidfuzz.utils import default_process\n",
    "from rapidfuzz.process import cdist, extract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3f85927f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define important variables\n",
    "corp_endings = {\n",
    "    'private limited', 'incorporation  inc', 'inc', 'llc', 'ltd', 'se', 'asa', 'as', \n",
    "    'gmbh', 'sas', 'llp', 's a', 'corporation', 'limited', 'corp', 'bv', 'lp', 'ab', \n",
    "    'sro', 'sp', 'spa', 'sár', 'sarl', 'ag', 'sa', 'co', 'sl', 'sàrl', 'complainant', \n",
    "    'of', 'sà rl', 'sa rl', 'a', 'plc', 'oyj', 'nv', 'akteingesellschaft', 'group', \n",
    "    'of', 'b v', 'e', 'm b', 'holdings', 'slu', 'respondent'\n",
    "}\n",
    "\n",
    "# helpful functions\n",
    "\n",
    "def parse_date(date_strings: List[str], dayfirst) -> datetime:\n",
    "    \"\"\"\n",
    "    Generates a date from the argument, by trying various heuristics and fixing common errors.\n",
    "\n",
    "    Args:\n",
    "        date_strings: List of candidate strings containing the desired date.\n",
    "        dayfirst: Whether to use the European date format (the day first) or the American one (month first)\n",
    "    \n",
    "    Returns:\n",
    "        A datetime object if a valid date was found.\n",
    "    \"\"\"\n",
    "    parsed_date = None\n",
    "    for _date in date_strings:\n",
    "        # print(_date)\n",
    "        _date = re.sub('\\s+', ' ', _date.replace('，', ', ').replace(',', ' , ').replace(';', ' ; ')).strip()\n",
    "        try:\n",
    "            parsed_date = parse(\n",
    "                _date, \n",
    "                # in some cases, the date starts with the year so the second check makes sure we\n",
    "                # are not considering the year as day which would lead to errors.\n",
    "                dayfirst = dayfirst and len(re.findall('[0-9]{1,4}', _date)[0]) <= 2, \n",
    "                fuzzy = True, \n",
    "                ignoretz=True\n",
    "            )\n",
    "            if f'{parsed_date.year}' in _date or f'{parsed_date.year}'[-2:] in _date:\n",
    "                break\n",
    "            else:\n",
    "                parsed_date = None\n",
    "        except:\n",
    "            continue\n",
    "    return parsed_date if parsed_date and parsed_date < datetime.now() and parsed_date.timestamp() > 0 else None\n",
    "\n",
    "\n",
    "def load_gpt_pair_map(infile: str) -> Dict[str, str]: \n",
    "    \"\"\"\n",
    "    Given an input JSON-formatted file that maps strings together based on their semantic meaning,\n",
    "    the function creates a dictionary representation for easy access.\n",
    "\n",
    "    Args:\n",
    "        infile: The path to the input file.\n",
    "    \n",
    "    Returns:\n",
    "        A mapping of source to destination strings.\n",
    "    \"\"\"\n",
    "    outDict = {}\n",
    "    with open(infile, 'rb') as fin:\n",
    "        for line in fin:\n",
    "            data = json.loads(line)\n",
    "            outDict[data[0]] = outDict.get(data[0], []) + [data[1]]\n",
    "    return outDict\n",
    "\n",
    "def deduplicate(names: List[str], score_cutoff = 1, refMapping: Dict[str, str] = {}) -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Given a list of names and a reference list, this function deduplicates all the names in the input.\n",
    "\n",
    "    Args:\n",
    "        names: The values to deduplicates\n",
    "        score_cutoff: The minimum similarity between two values before merging them.\n",
    "        refMapping: The reference list to be used for the mapping.\n",
    "    \n",
    "    Returns:\n",
    "        A mapping of input-string to their deduplicated version.\n",
    "    \"\"\"\n",
    "    names_ctr = Counter(names)\n",
    "    name_process = lambda x: re.sub(\n",
    "        '\\s+', \n",
    "        '', \n",
    "        re.sub(\n",
    "            fr\" ({'|'.join(corp_endings)})$\", \n",
    "            \"\", \n",
    "            default_process(unidecode(re.sub('(\\S)[./]', r'\\1',x)))\n",
    "        ).strip()\n",
    "    )\n",
    "    mx_names = names + list(refMapping.keys())\n",
    "    _names = [\n",
    "        re.sub(\n",
    "            # this is because some companies' names were represented as \"John Doe of ACME Corporation\"\n",
    "            'of$', \n",
    "            '', \n",
    "            re.sub('\\s+', ' ', unidecode(name))\n",
    "        ).strip().rstrip(',').strip().rstrip(',').strip() for name in mx_names\n",
    "    ]\n",
    "    # map the names between them\n",
    "    dist = cdist(\n",
    "        _names, _names, \n",
    "        scorer = Levenshtein.normalized_similarity, \n",
    "        processor = name_process, \n",
    "        score_cutoff = score_cutoff, \n",
    "        workers = -1\n",
    "    )\n",
    "\n",
    "    graph_dict = refMapping.copy()\n",
    "    for idx, jdx in zip(*np.nonzero(dist)):\n",
    "        graph_dict[mx_names[idx]] = graph_dict.get(mx_names[idx], []) + [mx_names[jdx]]\n",
    "    valsMap = {}\n",
    "\n",
    "    # use transitive relations for deduplications\n",
    "    G = nx.from_dict_of_lists(graph_dict)\n",
    "    for cc in nx.connected_components(G):\n",
    "        # find the name that represents the cluster best\n",
    "        repr_name = re.sub(\n",
    "            'of$', \n",
    "            '', \n",
    "            re.sub(\n",
    "                '\\s+', \n",
    "                ' ', \n",
    "                unidecode(max(cc, key = lambda x: names_ctr.get(x, 0)))\n",
    "            )\n",
    "        ).strip().rstrip(',').strip().rstrip(',').strip()\n",
    "        if name_process(repr_name) not in corp_endings.union(['']):\n",
    "            valsMap.update({val: repr_name for val in cc})\n",
    "    return valsMap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5597df86",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc8f97fee7514e53a45384d3b37360c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/12012 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/vinny/udrp-artefacts/.venv/lib/python3.10/site-packages/thinc/shims/pytorch.py:114: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(self._mixed_precision):\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1808 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "spacy.prefer_gpu(1)\n",
    "\n",
    "infile = '../data/Jan2023-Aug2024-disputes.jsonl.gz'\n",
    "outfile = '../data/Jan2023-Aug2024-parsed-proceedings.jsonl.gz'\n",
    "\n",
    "entities = [\n",
    "    'FILING_DATE', 'REGISTRAR_REQ_DATE', 'REGISTRAR_REQ_RESP', 'COMMENCEMENT_DATE',\n",
    "    'APPT_DATE', 'PUB_DATE', 'COMPLAINANT', 'COMP_LOC', 'REPR_ORG', 'RESPONDENT',\n",
    "    'RESP_LOC', 'NO_RESPONSE', 'PANELIST', 'REGISTRAR', 'TRADEMARK'\n",
    "]\n",
    "\n",
    "model_id = 'en_udrp_extractor_baseline'\n",
    "\n",
    "nlp = spacy.load(model_id)\n",
    "\n",
    "# load the file\n",
    "df = pd.read_json(infile, lines = True)\n",
    "\n",
    "out_arr = []\n",
    "\n",
    "# extract the features from the proceedings\n",
    "for case, doc in tqdm(zip(df.itertuples(index = False), nlp.pipe(df.text.tolist())), total = df.shape[0]):\n",
    "    record = {\n",
    "        'source': case.source, 'number': case.number, 'domains': case.domains, \n",
    "        'complainants': case.complainants, 'respondents': case.respondents, \n",
    "        'url': case.url, 'status': case.status, 'date': case.date\n",
    "    }\n",
    "    record.update({key: [] for key in entities})\n",
    "    for ent in doc.ents:\n",
    "        record[ent.label_] = record.get(ent.label_, []) + [ent.text]\n",
    "    out_arr.append(record)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "34874b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix the wordings of the final decisions and the providers' names\n",
    "df = pd.DataFrame(out_arr)\n",
    "\n",
    "transfer_opts = {\n",
    "    'Request granted', 'transferred', 'Accepted', 'Transferred', 'Transfer', 'Domain Name Transferred', 'Transfer, denied in part', 'Transfer with Accelerated Proceeding', \n",
    "    'Transfer, cancellation in part', 'Transfer with dissenting opinion', 'Transfer with concurring opinion', 'Transfer\\nTransfer', 'Transferred/ Dismissed', \n",
    "    'Transfer\\nTransfer\\nTransfer\\nTransfer', 'Transfered','\\nTransfer', 'Transfer\\xa0\\nTransfer', 'Transfer\\xa0', 'bytedancing.com 转移',\n",
    "    '域名争议', '转移争议域名给投诉人', '转移给投诉人1', 'Relief Granted', 'Cancellation, transfer in part', '转移', '域名转移', 'tranferred', '转移域名', \n",
    "}\n",
    "denial_opts = {\n",
    "    'rejected', 'dismissed', '驳回', 'Rejected\\nRejected', 'Dismissed', 'Complaint denied with dissenting and concurring opinion', 'Complaint denied with concurring opinion', \n",
    "    'denied', 'Dismiss', 'Request rejected', 'Denial with Accelerated Proceeding', \"Complaint denied\", 'Claim Denied', 'Rejected', 'Final', \n",
    "    'Complaint denied with dissenting opinion', 'Complaint Rejected'\n",
    "}\n",
    "withdrawn_opts = {\"Withdrawn\", 'Withdraw', 'Case Withdrawn', 'Withdrawn w/Decision'}\n",
    "terminated_opts = {\"Case terminated\", 'Terminated by Panel (order published)', '驳回投诉', 'Terminated settlement', 'Settled'}\n",
    "cancelled_opts = {\n",
    "    'Cancellation with dissenting opinion', 'cancelled', '撤销注册', 'Deleted', \"Cancellation\", 'Domain Name Cancelled', 'Split Determination', 'Cancel', '注销域名', '注销', \n",
    "    'Revoked', 'Revoke', '争议域名“thyzzendetail.com”予以注销。'\n",
    "}\n",
    "suspended_opts = {\"Default\"}\n",
    "pending_opts = {\"No decision\"}\n",
    "split_opts = {\n",
    "    'Complaint denied, transfer in part with dissenting opinion', 'Cancellation, denied in part', 'Transfer\\nTransfer\\nDenied\\nDenied\\nNo Dispute', \n",
    "    'Transfer, denied in part with dissenting opinion', 'Partial Denial', 'Complaint denied, transfer in part', 'Partially accepted', '支持diditrip.com, 驳回xiaojulvxing.com'\n",
    "}\n",
    "\n",
    "df.loc[df.status.isin(transfer_opts), 'status'] = 'Transferred'\n",
    "df.loc[df.status.isin(denial_opts), 'status'] = 'Denied'\n",
    "df.loc[df.status.isin(withdrawn_opts | terminated_opts), 'status'] = 'Terminated'\n",
    "# df.loc[df.status.isin(terminated_opts), 'status'] = 'Terminated'\n",
    "df.loc[df.status.isin(cancelled_opts), 'status'] = 'Cancelled'\n",
    "df.loc[df.status.isin(suspended_opts), 'status'] = 'Suspended'\n",
    "df.loc[df.status.isin(pending_opts), 'status'] = 'Pending'\n",
    "df.loc[df.status.isin(split_opts), 'status'] = 'Split Decision'\n",
    "df.loc[(df.status.isna()) & (df.url.isna()), 'status'] = 'Pending'\n",
    "# df.fillna({'status': 'Transferred'}, inplace = True)\n",
    "df.replace({'AIAC': 'ADNDRC', 'CIETAC-ODRC': 'ADNDRC', 'HKIAC': 'ADNDRC', 'IDRC': 'ADNDRC', 'Czech Arbitration Court': 'CAC'}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9b5374c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc425b70841f4c1d973765bb1dd6246a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Deduplication of registrars' names (map them to official names)\n",
    "\n",
    "# historical registrars information\n",
    "with open('../resources/historical-registrars-2013-12nov2024.csv', 'r', errors = 'replace') as fin:\n",
    "    csv_reader = csv.reader(fin)\n",
    "    icann_registrars = list(set([row[1] for row in csv_reader if row[2] != 'Reserved' and row[0] != '1'][1:]))\n",
    "\n",
    "# about the registrars\n",
    "registrar_sim_cutoff = 0.85\n",
    "registrarsMap = {}\n",
    "\n",
    "all_registrars = list(set().union(*df.REGISTRAR))\n",
    "\n",
    "# includes all known changes applied to a registrar's name\n",
    "reg_process = lambda x: re.sub(\n",
    "    '\\s+', ' ', \n",
    "    re.sub(\n",
    "        fr\" ({'|'.join(corp_endings)})$\", \n",
    "        \"\", \n",
    "        default_process(\n",
    "            re.split(\n",
    "                ' ([dD][/ ]*[Bb][/ ]*[Aa][/ ]*|[aA][/ ]*[Kk][/ ]*[Aa][/ ]*)', \n",
    "                x.replace('(the \"Registrar\"', '')\n",
    "            )[0].strip()\n",
    "        )\n",
    "    ).strip()\n",
    ")\n",
    "\n",
    "dist = cdist(\n",
    "    all_registrars, \n",
    "    icann_registrars, \n",
    "    scorer = Levenshtein.normalized_similarity, \n",
    "    score_cutoff = registrar_sim_cutoff, \n",
    "    processor = reg_process\n",
    ")\n",
    "\n",
    "# for cases where a corporate ending is added to the registrar's name\n",
    "process_wo_reg = lambda x: re.sub(\n",
    "    '\\s+', ' ', \n",
    "    re.sub(\n",
    "        fr\" ({'|'.join(corp_endings)})$\", \n",
    "        \"\", \n",
    "        default_process(re.sub('[Rr]egistrar', '', x.replace('(the \"Registrar\"', '')))\n",
    "    ).strip()\n",
    ")\n",
    "dist_cropped_reg = cdist(\n",
    "    all_registrars, \n",
    "    icann_registrars, \n",
    "    scorer = Levenshtein.normalized_similarity, \n",
    "    score_cutoff = registrar_sim_cutoff, \n",
    "    processor = process_wo_reg\n",
    ")\n",
    "\n",
    "# for cases where the registrar's name contains \"dba - doing business as\"\n",
    "process_alias = lambda x: re.sub(\n",
    "    '\\s+', \n",
    "    ' ', \n",
    "    re.sub(\n",
    "        fr\" ({'|'.join(corp_endings)})$\", \n",
    "        \"\", \n",
    "        default_process(\n",
    "            re.split(\n",
    "                ' [dD][/ ]*[Bb][/ ]*[Aa][/ ]*', \n",
    "                x.replace('(the \"Registrar\"', '')\n",
    "            )[-1].strip()\n",
    "        )\n",
    "    ).strip()\n",
    ")\n",
    "dist_alias = cdist(\n",
    "    all_registrars, \n",
    "    icann_registrars, \n",
    "    scorer = Levenshtein.normalized_similarity, \n",
    "    score_cutoff = registrar_sim_cutoff, \n",
    "    processor = process_alias)\n",
    "\n",
    "for idx, reg in tqdm(enumerate(all_registrars)):\n",
    "    subs = (\n",
    "        [(icann_registrars[jdx], float(val)) for jdx, val in enumerate(dist[idx, :]) if val > 0] or \n",
    "        [(icann_registrars[jdx], float(val)) for jdx, val in enumerate(dist_cropped_reg[idx, :]) if val > 0] or \n",
    "        [(icann_registrars[jdx], float(val)) for jdx, val in enumerate(dist_alias[idx, :]) if val > 0]\n",
    "    )\n",
    "    if subs:\n",
    "        registrarsMap[reg] = max(subs, key = lambda x: x[1])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5b36cc54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2bed6390e24549c5a18b55b96f5136a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# fix panelists' names\n",
    "\n",
    "# about the panelists\n",
    "all_panelists = list(set().union(*df.PANELIST))\n",
    "\n",
    "# reference NER model to find names\n",
    "ref_nlp = spacy.load('en_core_web_trf')\n",
    "\n",
    "panelistsName = {\n",
    "    doc.text: [\n",
    "        ent.text.replace('\\n', '').replace('/', '') \n",
    "        for ent in doc.ents if ent.label_ == \"PERSON\" and len(ent.text.split()) > 1\n",
    "    ]\n",
    "    for doc in tqdm(\n",
    "        ref_nlp.pipe([\n",
    "            panelist.replace('M s.', 'Ms.').replace(\"M, \", \"M. \").replace('\\n', '').replace('/', '') \n",
    "            for panelist in all_panelists\n",
    "        ])\n",
    "    )\n",
    "}\n",
    "\n",
    "panelCases = Counter()\n",
    "\n",
    "for panel in df.PANELIST.values:\n",
    "    panelCases.update(set().union(*[panelistsName.get(panelist, []) for panelist in panel]))\n",
    "\n",
    "# normalize the names\n",
    "panelist_process = lambda x: re.sub(\n",
    "    '\\s+', ' ', \n",
    "    re.sub(\n",
    "        '(esq|ret|qc|kc)$', \n",
    "        '', \n",
    "        default_process(re.sub('(\\S)[./]', r'\\1', unidecode(x)))\n",
    "    )\n",
    ").strip()\n",
    "\n",
    "# load the dictionary from the GPT-generated data\n",
    "# this list was generated using the prompt in \"chatgpt_prompts/mapping_panelists_names.txt\"\n",
    "gpt_names_dict = load_gpt_pair_map('../resources/gpt-panelists-names.jsonl')\n",
    "\n",
    "# create a mapping between the panelists' names\n",
    "namesDict = {\n",
    "    name: \n",
    "        # first, the GPT names\n",
    "        gpt_names_dict.get(name, []) + [\n",
    "        # then after pattern matching\n",
    "        val[0] for val in extract(name, list(panelCases.keys()), scorer = Levenshtein.distance, score_cutoff=0, limit = None, processor = panelist_process)\n",
    "    ] + [\n",
    "        # another set for matching when middle names are used\n",
    "        val[0] for val in extract(\n",
    "            name, \n",
    "            list(panelCases.keys()), \n",
    "            scorer = lambda x, y, score_cutoff = None: score_cutoff + 1 if ((x[0] == y[0] and x[-1] == y[-1])) else 0, \n",
    "            # there should be two parts of the name matching at least\n",
    "            score_cutoff = 2, \n",
    "            processor = lambda x: panelist_process(x).split()\n",
    "        )\n",
    "    ]\n",
    "    for name in panelCases\n",
    "}\n",
    "\n",
    "# use the transitive relationship to deduplicate. Use the name that appeared the most\n",
    "G = nx.from_dict_of_lists(namesDict)\n",
    "\n",
    "namesMap = {}\n",
    "for cc in nx.connected_components(G):\n",
    "    repr_val = max(cc, key = lambda x: panelCases.get(x, 0))\n",
    "    namesMap.update({val: repr_val for val in cc})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c1ab1df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mapping of addresses\n",
    "# this file was generated by passing the extracted addresses to Nominatim.\n",
    "# because the execution is very slow, we did not include that processing.\n",
    "# nonetheless, the supporting code is available under \n",
    "# code/retrieve_country_names.py\n",
    "\n",
    "with open('../resources/addresses-to-countries-fixed.json', 'r') as fin:\n",
    "    addressesToCountries = json.load(fin)\n",
    "\n",
    "with open('../resources/gpt-countries-found.json', 'r') as fin1, open('../resources/complainants-to-locations-sec-edgar.json', 'r') as fin2:\n",
    "    # this file was generated by using U.S. Securities and Exchange files, considering every company on that list as headquarted in the United States\n",
    "    complainantToCountry = json.load(fin2)\n",
    "\n",
    "    # this list was generated via ChatGPT using the prompt in chatgpt_prompts/mapping_parties_countries.txt\n",
    "    val2 = json.load(fin1)\n",
    "    \n",
    "    complainantToCountry.update(val2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4d00cbe0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/vinny/udrp-artefacts/.venv/lib/python3.10/site-packages/thinc/shims/pytorch.py:114: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(self._mixed_precision):\n"
     ]
    }
   ],
   "source": [
    "# deduplicate complainants' names\n",
    "# CAC systematically puts \"Organization\" in front of the complainants' names so we fix that\n",
    "# we want both the names as extracted from the public listing (column complainants) \n",
    "# and the one extracted from the proceedings (column COMPLAINANT)\n",
    "\n",
    "all_complainants = list(\n",
    "    set().union(\n",
    "        *[[\n",
    "            re.sub(r'^(Organization|Complainant) ', '' if row.source == \"CAC\" else 'Organization ', val) for val in row.COMPLAINANT\n",
    "        ] + [\n",
    "        complainant for complainant in row.complainants if complainant] \n",
    "        for row in df.itertuples(index = False)\n",
    "    ])\n",
    ")\n",
    "\n",
    "complainantNames = {\n",
    "    doc.text: [ent.text.strip() for ent in doc.ents if ent.label_ in {'ORG', 'PERSON'}] or [doc.text]\n",
    "    for doc in ref_nlp.pipe(all_complainants)\n",
    "}\n",
    "\n",
    "# this mapping was created through pattern matching with historical Fortune Magazine list.\n",
    "\n",
    "companies_map = load_gpt_pair_map('../resources/companies-names-matched-gpt-2.json')\n",
    "complainantsMap = deduplicate(list(set().union(*complainantNames.values())), refMapping = companies_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "58b3645b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/vinny/udrp-artefacts/.venv/lib/python3.10/site-packages/thinc/shims/pytorch.py:114: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(self._mixed_precision):\n"
     ]
    }
   ],
   "source": [
    "# representative\n",
    "all_representatives = list(set().union(*df['REPR_ORG']))\n",
    "\n",
    "representativeOrgs = {\n",
    "    doc.text: ([ent.text for ent in doc.ents if ent.label_ == 'ORG'] or [doc.text])[0]\n",
    "    for doc in nlp.pipe(all_representatives)\n",
    "}\n",
    "\n",
    "# this list was generated using the prompt in \"chatgpt_prompts/mapping_legal_representatives_names.txt\"\n",
    "gpt_repr_map = load_gpt_pair_map('../resources/gpt-fixed-law-firms.jsonl')\n",
    "\n",
    "representativesMap = deduplicate(list(representativeOrgs.values()), refMapping = gpt_repr_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0ee3011f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/vinny/udrp-artefacts/.venv/lib/python3.10/site-packages/thinc/shims/pytorch.py:114: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with torch.cuda.amp.autocast(self._mixed_precision):\n"
     ]
    }
   ],
   "source": [
    "# respondents\n",
    "# we only take the first respondent name because some files ended up with multiple names, leading to memory errors.\n",
    "all_respondents = list(set().union(*[row.RESPONDENT[:1] + [resp for resp in row.respondents if resp] for row in df.itertuples(index = False)]))\n",
    "\n",
    "respondentNames = {\n",
    "    doc.text: [ent.text for ent in doc.ents if ent.label_ in {'ORG', 'PERSON'}] or [doc.text]\n",
    "    for doc in ref_nlp.pipe(all_respondents)\n",
    "}\n",
    "respondentsMap = deduplicate(list(set().union(*respondentNames.values())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "02834b9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trademarks \n",
    "all_trademarks = list(set().union(*df.TRADEMARK))\n",
    "trademarksMap = deduplicate(all_trademarks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "e33d9da2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1388939152a428292628d34bafbb04f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# create a list of names that are common among panelists\n",
    "# names that appear in 1% of the panelists' names are considered stop-words.\n",
    "panelists_ds_names = set(namesMap.values())\n",
    "panelists_stop_names = {\n",
    "    panelist_process(name) \n",
    "    for name, cnt in Counter(\n",
    "        sum([el.split() for el in panelists_ds_names], [])).most_common() \n",
    "        if cnt >= 0.01 * len(panelists_ds_names)\n",
    "    }\n",
    "\n",
    "# we found the name \"alan\" to be quite common, leading to false deduplications\n",
    "panelists_stop_names.update(['alan'])\n",
    "\n",
    "with gzip.open(outfile, 'wt') as fout:\n",
    "    for row in tqdm(df.itertuples()):\n",
    "        # dayfirst is only False for FORUM\n",
    "        notforum = row.source != \"FORUM\"\n",
    "        filing_date = parse_date(row.FILING_DATE, dayfirst = notforum)\n",
    "        publication_date = parse_date(row.PUB_DATE, dayfirst = notforum) or (row.date.to_pydatetime() if not pd.isnull(row.date) else None)\n",
    "        appointment_date = parse_date(row.APPT_DATE, dayfirst = notforum)\n",
    "        commencement_date = parse_date(row.COMMENCEMENT_DATE, dayfirst = notforum)\n",
    "        registrar_req_date = parse_date(row.REGISTRAR_REQ_DATE, dayfirst = notforum)\n",
    "        registrar_resp_date = parse_date(row.REGISTRAR_REQ_RESP, dayfirst = notforum)\n",
    "\n",
    "        # deduplicate the names of the panelists in the case\n",
    "        _panelists = list(set().union(*[[namesMap[val] for val in panelistsName.get(panelist, [])] for panelist in row.PANELIST]))\n",
    "\n",
    "        casePanelNames = {}\n",
    "        if len(_panelists) > 1:\n",
    "            dist = cdist(\n",
    "                _panelists, _panelists, \n",
    "                scorer = Levenshtein.normalized_similarity, \n",
    "                processor = lambda x: [\n",
    "                    el for el in panelist_process(x).split() \n",
    "                    if len(el) > 1 and el not in panelists_stop_names\n",
    "                ] or [x]\n",
    "            )\n",
    "\n",
    "            p_dict = {}\n",
    "            for idx, jdx in zip(*np.nonzero(dist)):\n",
    "                if idx != jdx:\n",
    "                    p_dict[_panelists[idx]] = p_dict.get(_panelists[idx], []) + [_panelists[jdx]]\n",
    "\n",
    "            G = nx.from_dict_of_lists(p_dict)\n",
    "            for cc in nx.connected_components(G):\n",
    "                if len(cc):\n",
    "                    nv = max(cc, key = panelCases.get)\n",
    "                    casePanelNames.update({el: nv for el in cc})\n",
    "        panelists = {casePanelNames.get(name, name) for name in _panelists}\n",
    "        \n",
    "        # # find the registrars\n",
    "        registrars = {registrarsMap.get(registrar) for registrar in row.REGISTRAR}\n",
    "\n",
    "        if None in registrars:\n",
    "            registrars.remove(None)\n",
    "\n",
    "        # whether the respondent defaulted\n",
    "        default = len(row.NO_RESPONSE) > 0\n",
    "\n",
    "        # complainant's information\n",
    "        complainants = set().union(*[complainantNames.get(val, []) for val in row.COMPLAINANT])\n",
    "        respondents = set().union(*[respondentNames.get(val, []) for val in row.RESPONDENT])\n",
    "        representatives = [representativeOrgs[val] for val in row.REPR_ORG]\n",
    "        representative = representatives[0] if representatives else None\n",
    "\n",
    "        # case's information\n",
    "        trademarks = set(row.TRADEMARK)\n",
    "        \n",
    "        out_data = {\n",
    "            'number': row.number,\n",
    "            'status': row.status,\n",
    "            'source': row.source,\n",
    "            'url': row.url,\n",
    "            'submitted': filing_date.strftime(\"%Y-%m-%d %H:%M:%S\") if filing_date else None,\n",
    "            'commenced': commencement_date.strftime(\"%Y-%m-%d %H:%M:%S\") if commencement_date else None,\n",
    "            'panel_appointed': appointment_date.strftime(\"%Y-%m-%d %H:%M:%S\") if appointment_date else None,\n",
    "            'registrar_request_date': registrar_req_date.strftime(\"%Y-%m-%d %H:%M:%S\") if registrar_req_date else None,\n",
    "            'registrar_response_date': registrar_resp_date.strftime(\"%Y-%m-%d %H:%M:%S\") if registrar_resp_date else None,\n",
    "            'published': publication_date.strftime(\"%Y-%m-%d %H:%M:%S\") if publication_date else None,\n",
    "            'registrars': list(registrars),\n",
    "            'panelists': list(panelists),\n",
    "            'registrant_default': default,\n",
    "            'complainants': list({complainantsMap[complainant] for complainant in complainants if complainantsMap.get(complainant)}),\n",
    "            'represented_by': representativesMap.get(representative),\n",
    "            'complainants_countries': list(\n",
    "                set().union(\n",
    "                    *(\n",
    "                        [addressesToCountries.get(address, []) for address in row.COMP_LOC] or \n",
    "                        [[complainantToCountry[complainant]] for complainant in complainants if complainant in complainantToCountry]\n",
    "                    )\n",
    "                )),\n",
    "            'respondents': list({respondentsMap[respondent] for respondent in respondents if respondentsMap.get(respondent)}),\n",
    "            'respondents_countries': list(set().union(*[addressesToCountries.get(address, []) for address in row.RESP_LOC])),\n",
    "            'domains': row.domains,\n",
    "            'trademarks': list({trademarksMap[trademark] for trademark in trademarks if trademarksMap.get(trademark)})\n",
    "        }\n",
    "\n",
    "        fout.write(json.dumps(out_data) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b8170eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>number</th>\n",
       "      <th>status</th>\n",
       "      <th>source</th>\n",
       "      <th>url</th>\n",
       "      <th>submitted</th>\n",
       "      <th>commenced</th>\n",
       "      <th>panel_appointed</th>\n",
       "      <th>registrar_request_date</th>\n",
       "      <th>registrar_response_date</th>\n",
       "      <th>published</th>\n",
       "      <th>registrars</th>\n",
       "      <th>panelists</th>\n",
       "      <th>registrant_default</th>\n",
       "      <th>complainants</th>\n",
       "      <th>represented_by</th>\n",
       "      <th>complainants_countries</th>\n",
       "      <th>respondents</th>\n",
       "      <th>respondents_countries</th>\n",
       "      <th>domains</th>\n",
       "      <th>trademarks</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CN-2401600</td>\n",
       "      <td>Transferred</td>\n",
       "      <td>ADNDRC</td>\n",
       "      <td>http://odr.org.cn/superadmin/downLoadFile.acti...</td>\n",
       "      <td>2024-01-09 00:00:00</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>2024-01-11 00:00:00</td>\n",
       "      <td>2024-03-04 00:00:00</td>\n",
       "      <td>[Web Commerce Communications Limited dba WebNi...</td>\n",
       "      <td>[Kun FAN]</td>\n",
       "      <td>True</td>\n",
       "      <td>[Midea Group Co., Ltd.]</td>\n",
       "      <td>Liu, Shen &amp; Associates.</td>\n",
       "      <td>[]</td>\n",
       "      <td>[Benjamin Smith]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[mideacapitalholdings.com]</td>\n",
       "      <td>[MIDEA]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>19507-UDRP</td>\n",
       "      <td>Transferred</td>\n",
       "      <td>CIIDRC</td>\n",
       "      <td>https://ciidrc.org/wp-content/uploads/2023/02/...</td>\n",
       "      <td>2023-01-03 00:00:00</td>\n",
       "      <td>None</td>\n",
       "      <td>2023-01-05 00:00:00</td>\n",
       "      <td>None</td>\n",
       "      <td>2023-01-05 00:00:00</td>\n",
       "      <td>2023-02-14 00:00:00</td>\n",
       "      <td>[]</td>\n",
       "      <td>[Marko Kevic, Claire Kowarsky]</td>\n",
       "      <td>True</td>\n",
       "      <td>[Traffic Tech Inc.]</td>\n",
       "      <td>None</td>\n",
       "      <td>[Canada]</td>\n",
       "      <td>[Claire Kowarsky, Adam Paris]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[trafflctech.com]</td>\n",
       "      <td>[TRAFFIC TECH, TRAFFIC, TECH]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19940-UDRP</td>\n",
       "      <td>Transferred</td>\n",
       "      <td>CIIDRC</td>\n",
       "      <td>https://ciidrc.org/wp-content/uploads/2022/12/...</td>\n",
       "      <td>2023-01-31 00:00:00</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>2022-12-30 00:00:00</td>\n",
       "      <td>2022-02-21 00:00:00</td>\n",
       "      <td>[]</td>\n",
       "      <td>[Rosemarie Sarrazin, Craig Chiasson]</td>\n",
       "      <td>False</td>\n",
       "      <td>[Altra Foods Inc.]</td>\n",
       "      <td>Rosemarie Sarrazin of Miller Thompson</td>\n",
       "      <td>[None]</td>\n",
       "      <td>[Craig Chiasson]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[altrafoods.com]</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>20014-UDRP</td>\n",
       "      <td>Denied</td>\n",
       "      <td>CIIDRC</td>\n",
       "      <td>https://ciidrc.org/wp-content/uploads/2023/02/...</td>\n",
       "      <td>2023-01-23 00:00:00</td>\n",
       "      <td>2023-01-26 00:00:00</td>\n",
       "      <td>2023-02-24 00:00:00</td>\n",
       "      <td>2023-02-06 00:00:00</td>\n",
       "      <td>None</td>\n",
       "      <td>2023-03-06 00:00:00</td>\n",
       "      <td>[]</td>\n",
       "      <td>[Fabrizio Bedarida, Gustavo Moser, Steven M. L...</td>\n",
       "      <td>False</td>\n",
       "      <td>[Reinvent LTD]</td>\n",
       "      <td>Ho Kyoung Trading Co. , Ltd.</td>\n",
       "      <td>[None]</td>\n",
       "      <td>[Fabrizio Bedarida, Gustavo Moser, Steven M. L...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[casinoin.com]</td>\n",
       "      <td>[CASINOIN]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20348-UDRP</td>\n",
       "      <td>Denied</td>\n",
       "      <td>CIIDRC</td>\n",
       "      <td>https://ciidrc.org/wp-content/uploads/2023/03/...</td>\n",
       "      <td>2023-02-13 00:00:00</td>\n",
       "      <td>2023-03-10 00:00:00</td>\n",
       "      <td>2023-03-24 00:00:00</td>\n",
       "      <td>None</td>\n",
       "      <td>2023-02-13 00:00:00</td>\n",
       "      <td>2023-04-04 00:00:00</td>\n",
       "      <td>[]</td>\n",
       "      <td>[Zak Muscovitch, Peter Müller]</td>\n",
       "      <td>False</td>\n",
       "      <td>[Paragon Protection Ltd.]</td>\n",
       "      <td>Zak Muscovitch, Michael Erdle</td>\n",
       "      <td>[None]</td>\n",
       "      <td>[Peter Muller]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[paragonsecurity.com]</td>\n",
       "      <td>[PARAGON SECURITY]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       number       status  source  \\\n",
       "0  CN-2401600  Transferred  ADNDRC   \n",
       "1  19507-UDRP  Transferred  CIIDRC   \n",
       "2  19940-UDRP  Transferred  CIIDRC   \n",
       "3  20014-UDRP       Denied  CIIDRC   \n",
       "4  20348-UDRP       Denied  CIIDRC   \n",
       "\n",
       "                                                 url            submitted  \\\n",
       "0  http://odr.org.cn/superadmin/downLoadFile.acti...  2024-01-09 00:00:00   \n",
       "1  https://ciidrc.org/wp-content/uploads/2023/02/...  2023-01-03 00:00:00   \n",
       "2  https://ciidrc.org/wp-content/uploads/2022/12/...  2023-01-31 00:00:00   \n",
       "3  https://ciidrc.org/wp-content/uploads/2023/02/...  2023-01-23 00:00:00   \n",
       "4  https://ciidrc.org/wp-content/uploads/2023/03/...  2023-02-13 00:00:00   \n",
       "\n",
       "             commenced      panel_appointed registrar_request_date  \\\n",
       "0                 None                 None                   None   \n",
       "1                 None  2023-01-05 00:00:00                   None   \n",
       "2                 None                 None                   None   \n",
       "3  2023-01-26 00:00:00  2023-02-24 00:00:00    2023-02-06 00:00:00   \n",
       "4  2023-03-10 00:00:00  2023-03-24 00:00:00                   None   \n",
       "\n",
       "  registrar_response_date            published  \\\n",
       "0     2024-01-11 00:00:00  2024-03-04 00:00:00   \n",
       "1     2023-01-05 00:00:00  2023-02-14 00:00:00   \n",
       "2     2022-12-30 00:00:00  2022-02-21 00:00:00   \n",
       "3                    None  2023-03-06 00:00:00   \n",
       "4     2023-02-13 00:00:00  2023-04-04 00:00:00   \n",
       "\n",
       "                                          registrars  \\\n",
       "0  [Web Commerce Communications Limited dba WebNi...   \n",
       "1                                                 []   \n",
       "2                                                 []   \n",
       "3                                                 []   \n",
       "4                                                 []   \n",
       "\n",
       "                                           panelists  registrant_default  \\\n",
       "0                                          [Kun FAN]                True   \n",
       "1                     [Marko Kevic, Claire Kowarsky]                True   \n",
       "2               [Rosemarie Sarrazin, Craig Chiasson]               False   \n",
       "3  [Fabrizio Bedarida, Gustavo Moser, Steven M. L...               False   \n",
       "4                     [Zak Muscovitch, Peter Müller]               False   \n",
       "\n",
       "                complainants                         represented_by  \\\n",
       "0    [Midea Group Co., Ltd.]                Liu, Shen & Associates.   \n",
       "1        [Traffic Tech Inc.]                                   None   \n",
       "2         [Altra Foods Inc.]  Rosemarie Sarrazin of Miller Thompson   \n",
       "3             [Reinvent LTD]           Ho Kyoung Trading Co. , Ltd.   \n",
       "4  [Paragon Protection Ltd.]          Zak Muscovitch, Michael Erdle   \n",
       "\n",
       "  complainants_countries                                        respondents  \\\n",
       "0                     []                                   [Benjamin Smith]   \n",
       "1               [Canada]                      [Claire Kowarsky, Adam Paris]   \n",
       "2                 [None]                                   [Craig Chiasson]   \n",
       "3                 [None]  [Fabrizio Bedarida, Gustavo Moser, Steven M. L...   \n",
       "4                 [None]                                     [Peter Muller]   \n",
       "\n",
       "  respondents_countries                     domains  \\\n",
       "0                    []  [mideacapitalholdings.com]   \n",
       "1                    []           [trafflctech.com]   \n",
       "2                    []            [altrafoods.com]   \n",
       "3                    []              [casinoin.com]   \n",
       "4                    []       [paragonsecurity.com]   \n",
       "\n",
       "                      trademarks  \n",
       "0                        [MIDEA]  \n",
       "1  [TRAFFIC TECH, TRAFFIC, TECH]  \n",
       "2                             []  \n",
       "3                     [CASINOIN]  \n",
       "4             [PARAGON SECURITY]  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "outfile = '../data/Jan2023-Aug2024-parsed-proceedings.jsonl.gz'\n",
    "\n",
    "genDf = pd.read_json(outfile, lines = True)\n",
    "\n",
    "genDf.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
